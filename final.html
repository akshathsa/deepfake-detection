<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>
  
  
    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="deepfakes, computer vision, speech recognition">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  
  
    <title>Team 6 Project Proposal</title>
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
  
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>


<!-- Paper Intro -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction/Background</h2>
        <div class="content has-text-justified">
          <p>
            "Deepfakes" encompasses a wide range of media created from the manipulation of existing sources and/or the creation of new content using advanced technologies. They can be used for malicious purposes, motivating the need to be able to identify deepfakes [1].
          </p>
          <p>
            Some approaches to deepfake detection include machine learning-based methods, deep learning-based methods (~77% of all research), statistical-based methods, and blockchain-based methods [2]. The main process of detecting deepfakes entails two parts: assessing how the face is integrated with its background and how the background blends with the face [3].
          </p>
          <p>
            We will primarily use the <a href="https://www.kaggle.com/c/deepfake-detection-challenge/data">DFDC dataset</a>, which includes deepfakes involving the manipulation of human facial images and voices [4].
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Intro -->
  
  <!-- Problem Definition -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Definition</h2>
        <div class="content has-text-justified">
          <p>
            The growing proliferation of deepfake technology poses various risks, including identity theft and the spread of misinformation. Our team’s motivation thus stems from the dire necessity for people to reliably distinguish between fiction and reality. By developing an effective model that can classify, with high AUC/ROC, whether videos are authentic or AI-generated, we aim to combat the aforementioned issues.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Problem Definition -->

 <!-- EDA -->
 <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Exploratory Data Analysis and Visualizations</h2>
          <div class="content has-text-justified">
            <h5 class="title">Dataset</h5>
            <p>
              The dataset used in this project was sourced from the <a href="https://www.kaggle.com/competitions/deepfake-detection-challenge/data" style="color: blue;">Deepfake Detection Challenge (DFDC)</a>
              on Kaggle [4]. More specifically, our team focused on a smaller subset of the data, which was around 10GB, whereas the entire dataset was a whopping 472GB, far too big for the scale of this project.
            </p>
            <h5 class="title">General Data Exploration</h5>
            <p>
                Let us start by analyzing the distribution of the dataset. From Figure 1, we can see that we have a skewed dataset
                with mostly fake videos. This is because the fake videos in this dataset were originally generated from real videos, therefore
                for each real video, there are potentially multiple fake videos that were generated using the real as a base.
                By analyzing the dataset, we were able to find out that each real video was used to generate, on average, 4.4 fake videos. 
            </p>
            <figure>
                <img src="./static/images/data_prop.png" alt="Data Prop" style="width:50%">
                <figcaption>Figure 1 - Proportion of each class in the dataset</figcaption>
            </figure>
            <h5 class="title">Single Frame Exploration</h5>
            <p>
              Let us take a closer look at the dataset now, by extracting specific frames of the video.
              Below are three frames from randomly chosen videos from each class.
          </p>
          <figure>
              <img src="./static/images/reals.png" alt="Reals" style="width:100%">
              <figcaption>Figure 2 - Single frames from real videos</figcaption>
          </figure>
          <figure>
            <img src="./static/images/fakes.png" alt="Fakes" style="width:100%">
            <figcaption>Figure 3 - Single frames from fake videos</figcaption>
          </figure>
          <h5 class="title">Consecutive Multi Frame Exploration</h5>
          <p>
            Unfortunately, we are not able to tell much from still images. Where AI struggles is maintaining consistency across frames in a video.
            Let us now slice into the videos and analyze a subset of frames at a time. To the left is a real video, whereas the right one is fake.
            Just from here, we are able to notice quite a bit. For instance, the face in the fake video is blurry and lacks detail, to the point
            where it even cuts off the person's glasses.
          </p>
          <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
          <video width="400" height="250" controls>
            <source src="./static/videos/real.mp4" type="video/mp4">
          </video>
          <video width="400" height="250" controls>
            <source src="./static/videos/fake.mp4" type="video/mp4">
          </video>
        </div>
        <h5 class="title" style="margin-top: 25px;">Haar Cascade Face Detection and Exploration</h5>
        <p>
          We can see that most of the inconsistency is around the face, which is usually the most difficult for an AI to get right. Let us zoom into the 
          faces in these videos. To do this, we will utilize an object detection algorithm called Haar Cascades Classifier.
        </p>
        <p>
          Notice the variations and expressiveness of the real video, as compared to the blank and emotionless face in the fake one.
        </p>
        <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
          <video width="400" height="250" controls>
            <source src="./static/videos/real_face.mp4" type="video/mp4">
          </video>
          <video width="400" height="250" controls>
            <source src="./static/videos/fake_face.mp4" type="video/mp4">
          </video>
        </div>
        <h5 class="title" style="margin-top: 25px;">Structural Similarity (SSIM) Index</h5>
        <p>
          Now that we know this fact about inconsistency, let us try to quantify it by making use of a metric called the Structural Similarity (SSIM) Index.
          Unfortunately, there does not seem to be a clear indicator that fake videos have a lower SSIM score, as both the real and fake ones seem to be quite random.
        </p>
        <figure>
          <img src="./static/images/ssim.png" alt="SSIM" style="width:100%">
          <figcaption>Figure 4 - SSIM score between consecutive frames</figcaption>
        </figure>
        <p>
          Now that we have explored the dataset, we can start preprocessing the videos to be trained.
          We will look into that in the next section.
        </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End EDA -->
  
  <!-- Methods -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>
            Our final project submission will include three separate methods for data preprocessing. The first method involves the use of Mask R-CNN to detect people in the video. The second method involves the use of convolutional autoencoders to reduce the dimensionality of the data. The third method involves performing data augmentation through minor video transformations that do not constitute deepfakes.
          </p>
          <h5 class="title">Mask R-CNN Implementation</h5>
          <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
            <video width="400" height="250" controls>
              <source src="./static/videos/maskrcnn_before.mp4" type="video/mp4">
            </video>
            <video width="250" height="160" controls>
              <source src="./static/videos/maskrcnn_after.mp4" type="video/mp4">
            </video>
          </div>
          <br>
          <p>
            For the midterm checkpoint, we have implemented the first of these pre-processing techniques. To reduce dataset size, we utilized PyTorch's implementation of Mask R-CNN (torchvision.models.detection.mask_rcnn.MaskRCNN). This allowed us to detect areas of frames in which people are present via pixel-level object segmentation and crop the frames accordingly. We used the videos that were generated using these cropped frames as input to our deepfake detection models. The Mask R-CNN model was initialized with weights obtained by pre-training on the COCO dataset. A sample of the video output (mask and bounding box included for reference) of the Mask R-CNN model is shown below. The video on the left is the original video, and the video on the right is the output of the Mask R-CNN model. On this particular video, the Mask R-CNN model yielded a 75% reduction in size.
          </p>
          <p>
            Our final project submission will include the implementation of three different deepfake classification techniques. The first involves the use of a support vector machine (SVM) trained on features extracted by feature point detectors. The second involves the use of a convolutional neural network (CNN) architecture called EfficientNet combined with a Vision Transformer. We will also implement a CNN ensemble model that utilizes two different CNN architectures combined with a meta-learner.
          </p>
          <h5 class="title">Support Vector Machine Implementation</h5>
          <figure>
            <img src="./static/images/flowchat.png" style="width:100%">
            <figcaption>Figure 5 - Flowchart of SVM process</figcaption>
          </figure>
          <p>
            For our first model, we utilized a support vector machine [5]. Once we finished the model, we created a visualization to better illustrate the steps and our data processing. We incorporated a flowchart that shows the process of the data preprocessing to feeding the information into the model.
          </p>
          <p>
            The scikit-learn SVM module includes multiple Support Vector Machine algorithms such as Linear SVM, Nu-Spport Vector and C-Support Vector classification. We chose to use C-Support Vector classification with a radial basis function (RBF) kernel for our implementation. The C parameter is a regularization parameter and can be tuned to manage the trade-off between smooth transition boundaries and accurate categorization of training samples. There are several other hyperparameters such as degree, which limit the degree of the polynomial kernel function. We used the default value of 3 for degree. We used the default value of "scale" for the gamma hyperparameter, which controls the kernel coefficient. For completeness, the other model hyperparameters that can be tuned include coef0, shrinking, tol, cache_size, class_weight, max_iter, decision_function_shape, break_ties, and random_state. The results of the model are presented in the following section.
          </p>
          <h5 class="title">EfficientNet-ViT Implementation</h5>
          <p>
            We are also in the process of implementing EfficientNet combined with a Vision Transformer (ViT) [6]. We pivoted from our original decision to implement a model solely based on ViTs owing to the need to capture localized information in video frames. In particular, we chose to use EfficientNet owing to the ability of convolutional neural networks to detect patch abnormalities in frames. EfficientNet processes the input video frames by extracting features for each 7x7 block in a particular frame. Following linear projection, ViT processes the features for each location generated by EfficientNet [6]. We used the smallest model in the EfficientNet family (EfficientNet B0). We leveraged PyTorch's implementation of this model (torchvision.models.efficientnet_b0) and initialized it with pre-trained weights for the training process. The last few layers of the model were fine-tuned on a subset of the DFDC dataset. The ViT portion of the model was implemented using PyTorch and NumPy. The CLS token is used prior to passing extracted features to the MLP head in order to generate a binary classification score.
          </p>
          <p>
            The model was trained in a supervised manner using 100 labeled training examples from DFDC, from which 30 frames were extracted per video. The specific hyperparameters used for model training are presented in the following table. The validation and testing sets were also subsets of DFDC. The inference procedure and results are presented in the following section.
          </p>
          <table class="styled-table">
            <tr>
              <th>Hyperparameter</th>
              <th>Value</th>
            </tr>
            <tr>
              <td>Batch Size</td>
              <td>1</td>
            </tr>
            <tr>
              <td>Learning Rate</td>
              <td>0.001</td>
            </tr>
            <tr>
              <td>Optimizer</td>
              <td>SGD</td>
            </tr>
            <tr>
              <td>Epochs</td>
              <td>20</td>
            </tr>
            <tr>
              <td>Frames Per Video</td>
              <td>30</td>
            </tr>
            <tr>
              <td>Image Size</td>
              <td>224</td>
            </tr>
            <tr>
              <td>Patch Size</td>
              <td>7</td>
            </tr>
            <tr>
              <td>Number of Heads</td>
              <td>8</td>
            </tr>
          </table>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Methods -->
  
   <!-- Results and Discussion -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results and Discussion</h2>
        <div class="content has-text-justified">
          <p>
            Based on how widely various metrics are used for this task, we have chosen the following metrics to evaluate model performance [2]:
          </p>
          <ol>
            <li>Area under the ROC curve evaluates binary classifier performance across different classification thresholds [1]. It also quantifies how well the model can differentiate classes [7].</li>
            <li>Precision measures the proportion of positive results within the positive predictions, ensuring that there are few false positives [8]. </li>
            <li>Log loss increases as predictive probability diverges from the actual label [9].</li>
          </ol>
          <p>
            Goal: We aim to detect whether a video is a deepfake accurately by maximizing the first two metrics and minimizing the third.
          </p>
          <p>
            Results: This project expects high accuracy rates and reduction of false positives, where genuine videos are classified as deep fakes. We expect to minimize false negatives, where deepfakes are misclassified as genuine videos.
          </p>	
          <h5 class="title">SVM Results</h5>
          <figure>
            <img src="./static/images/ROC-SVM.png" alt="ROC" style="width:100%">
            <figcaption>Figure 6 - ROC curve for SVM</figcaption>
          </figure>
          <figure>
            <img src="./static/images/confusion-matrix-svm.png" alt="confusion-matrix" style="width:100%">
            <figcaption>Figure 7 - Confusion-Matrix for SVM</figcaption>
          </figure>	
          <p>
            We achieved an AUC of 0.289 and a log loss of 0.70. The low AUC suggests the model cannot distinguish between fake and real videos. This is additionally demonstrated in the confusion matrix. The model is not yet performing well since the accuracy is only 59%. The model requires further hyperparameter tuning and an increase in the training dataset size. For instance, when we used 50 videos and split them into 80% training (40 training videos) and 20% testing, the model produced an accuracy of 54.29%. However, when we increased the number of training videos to 60 and split the dataset to 70% training (42 training videos) and 30% testing, the accuracy shot up to 59%. This shows that with even a slight increase in training data, the accuracy improves drastically.
          </p>
          <h5 class="title">EfficientNet-ViT Results</h5>
          <figure>
            <img src="./static/images/efficientnet_auc.jpg" alt="AUC" style="width:100%">
            <figcaption>Figure 8 - ROC curve for EfficientNet-ViT</figcaption>
          </figure>
          <figure>
            <img src="./static/images/efficientnet_prcurve.jpg" alt="PRC" style="width:100%">
            <figcaption>Figure 9 - Precision-Recall curve for EfficientNet-ViT</figcaption>
          </figure>
          <p>
            For inference for the EfficientNet-ViT model, we used a voting mechanism as discussed by Coccomini et al. [6]. However, rather than grouping scores by actor identifiers, we averaged the ratings on the individual faces in a given video to determine its label. The hard voting approach discussed by Coccomini et al. may be worth exploring for the final project submission [6]. The precision of the model was 0.6364 and the F1 score was 0.7778. The log loss was 0.6769 and the AUC was 0.321. The low precision of the model is likely due to the small training set used for the midterm checkpoint. Additionally, the AUC of 0.321 indicates that the model is currently performing worse than random guessing. The model currently tends toward false positives, which is reflected in the low AUC. The ROC curve and precision-recall curve for the model are shown below. We are confident that training the model on the entire DFDC dataset will help the model better classify negative examples and improve these metrics.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Results and Discussion -->

  <!-- Next Steps -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Next Steps</h2>
          <div class="content has-text-justified">
            <p>
              Our next steps include implementing our third model, the CNN ensemble model with a meta-learner. The new model that we plan to incorporate will require the implementation of the aforementioned data preprocessing techniques as well as a few additional ones in order to modify the input for the models that will be used in the ensemble. The models we plan to train and include in our ensemble will be the InceptionResNetV2 model and the DenseNet121 model [10]. These two models are highly compatible with each other. The InceptionResNetV2 model will handle analyzing the overall image for large incongruencies, whereas the DenseNet121 will look at the image more indepth for subtleties that allude to the video being a deepfake. Once these models are trained, we plan to utilize a meta-learner to efficiently combine the data from the two models to provide an accurate prediction. The model we will be using to complete this will be the LightGBM gradient boosting model [11]. We have decided on this model owing to its ability to handle large datasets quickly and effectively. Based on this implementation, we should be able to provide a CNN ensemble model with a meta learner that performs well on all the discussed metrics.
            </p>
            <p>
							We also plan to utilize stacked denoising autoencoders to yield better representations of the inputs. Autoencoders are neural nets that are usually trained to reduce the dimensionality of inputs and then use an inverse transform to recover an output that is as close to the input as possible. This process allows for the learning of a condensed representation of the data. Stacking multiple of these layers improves this process. While autoencoders are usually used to reduce the dimensionality of the dataset, denoising autoencoders learn a higher dimensional representation. This introduces noise, and denoising autoencoders leverage the process of reconstructing the input through the noise to gain a better understanding of the data [12].
            </p>
            <p>
              As far as the SVM implementation goes, personal compute resources limited the training data size. Using PACE ICE will reduce the training time and allow the model to handle a larger training dataset to improve its performance. When we saved the trained model to disk in pickle format, it occupied more than 24 GB. This explains how resource intensive these experiments were. By using dedicated compute machines, the process could be hastened significantly. 
            </p>
            <p>
              There are also plenty of improvements to make to the EfficientNet-ViT implementation and training pipeline. Specifically, owing to deadline constraints for the midterm, a very small subset of DFDC was used for training. We plan to train this model on PACE ICE as well using the entire 10GB dataset of videos (and, perhaps, an even larger subset of DFDC) for the final project submission and are confident that this will drastically improve model performance. In addition, the current setup involves passing in the features to the ViT component without any temporal information. We plan to modify the training procedure to incorporate such information so that the ViT can more effectively incorporate long-range dependencies.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Next Steps -->
  
   <!-- References -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">References</h2>
        <div class="content has-text-justified">
          <p>
            [1] E. Altuncu, V. Franqueira and S. Li, "Deepfake: Definitions, Performance Metrics and Standards, Datasets and Benchmarks, and a Meta-Review," arXiv:2208.10913 [cs.CV].
          </p>
          <p>
            [2] M. S. Rana, M. N. Nobi, B. Murali and A. H. Sung, "Deepfake Detection: A Systematic Literature Review," in IEEE Access, vol. 10, pp. 25494-25513, 2022, doi: 10.1109/ACCESS.2022.3154404.  
          </p>
          <p>
            [3] Y. Nirkin, L. Wolf, Y. Keller and T. Hassner, "DeepFake Detection Based on Discrepancies Between Faces and Their Context," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 6111-6121, 1 Oct. 2022, doi: 10.1109/TPAMI.2021.3093446.  
          </p>
          <p>
            [4] Brian Dolhansky, et al., "The DeepFake Detection Challenge (DFDC) Dataset," arXiv:2006.07397 [cs.CV].
          </p>
          <p>
            [5] F. F. Kharbat, T. Elamsy, A. Mahmoud and R. Abdullah, "Image Feature Detectors for Deepfake Video Detection," 2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA), Abu Dhabi, United Arab Emirates, 2019, pp. 1-4, doi: 10.1109/AICCSA47632.2019.9035360.
          </p>
          <p>
            [6] D. A. Coccomini, N. Messina, C. Gennaro, and F. Falchi, “Combining EfficientNet and vision transformers for video deepfake detection,” Image Analysis and Processing - ICIAP 2022, pp. 219-229, 2022. doi:10.1007/978-3-031-06433-3_19 
          </p>
          <p>
						[7] M. Norton and S. Uryasev, "Maximization of AUC and Buffered AUC in binary classification," Mathematical Programming, vol. 174, no. 1-2, pp. 575-612, Jul. 2018, doi: https://doi.org/10.1007/s10107-018-1312-2.
					</p>
          <p>
						[8] M. Groh , Z. Epstein, C. Firestone, and R. Picard, "Deepfake detection by human crowds, machines, and machine-informed crowds," pnas, https://www.pnas.org/doi/10.1073/pnas.2110013119.
					</p>
					<p>
						[9] S. A. Khan and D.-T. Dang-Nguyen, "Deepfake Detection: A Comparative Analysis," arXiv.org e-Print archive, https://arxiv.org/.
					</p>
          <p>
            [10] Md. S. Rana and A. H. Sung, “DeepfakeStack: A Deep Ensemble-based learning technique for deepfake detection,” 2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom), Aug. 2020. doi:10.1109/cscloud-edgecom49738.2020.00021 
          </p>
          <p>
            [11] K. Yao, J. Wang, B. Diao, and C. Li, “Towards understanding the generalization of deepfake detectors from a game-theoretical view,” 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Oct. 2023. doi:10.1109/iccv51070.2023.00194 
          </p>
          <p>
            [12] J. Liang and K. Kelly, "Training Stacked Denoising Autoencoders for Representation Learning," arXiv:2102.08012 [cs.CV].
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End References -->
  
  <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Contribution Table</h2>
      <div class="columns is-centered has-text-centered">
  <table class="styled-table">
  <tr>
    <th>Name</th>
    <th>Final Contributions</th>
  </tr>
  <tr>
    <td>Sagar Sadak</td>
    <td>Exploratory Data Analysis and Visualization</td>
  </tr>
  <tr>
    <td>Shaam Bala</td>
    <td>Introduction, Problem Description, Next Steps, Visualizations, CNN Ensemble Model with Meta-learner Implementation</td>
  </tr>
    <tr>
    <td>Akaash Parthasarathy</td>
    <td>Methods, Next Steps, EfficientNet-ViT Implementation and Discussion, Mask R-CNN Implementation</td>
  </tr>
    <tr>
    <td>Akshath Shvetang Anna</td>
    <td>Next Steps, Stacked Denoising Autoencoders Implementation</td>
  </tr>
    <tr>
    <td>Harish Rajan</td>
    <td>SVM Implementation and Visualizations, Results and Discussion for SVM</td>
  </tr>
</table>
      </div>
    </div>
  </div>
</section>
</body>
</html>
