<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>
  
  
    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="deepfakes, computer vision, speech recognition">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  
  
    <title>Team 6 Project Proposal</title>
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
  
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>


<!-- Paper Intro -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction/Background</h2>
        <div class="content has-text-justified">
          <p>
            "Deepfakes" encompasses a wide range of media created from the manipulation of existing sources and/or the creation of new content using advanced technologies. They can be used for malicious purposes, motivating the need to be able to identify deepfakes [1].
          </p>
          <p>
            Some approaches to deepfake detection include machine learning-based methods, deep learning-based methods (~77% of all research), statistical-based methods, and blockchain-based methods [2]. The main process of detecting deepfakes entails two parts: assessing how the face is integrated with its background and how the background blends with the face [3].
          </p>
          <p>
            We will primarily use the <a href="https://www.kaggle.com/c/deepfake-detection-challenge/data">DFDC dataset</a>, which includes deepfakes involving the manipulation of human facial images and voices [4].
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Intro -->
  
  <!-- Problem Definition -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Definition</h2>
        <div class="content has-text-justified">
          <p>
            The growing proliferation of deepfake technology poses various risks, including identity theft and the spread of misinformation. Our team’s motivation thus stems from the dire necessity for people to reliably distinguish between fiction and reality. By developing an effective model that can classify, with high AUC/ROC, whether videos are authentic or AI-generated, we aim to combat the aforementioned issues.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Problem Definition -->

 <!-- EDA -->
 <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Exploratory Data Analysis and Visualizations</h2>
          <div class="content has-text-justified">
            <h5 class="title">Dataset</h5>
            <p>
              The dataset used in this project was sourced from the <a href="https://www.kaggle.com/competitions/deepfake-detection-challenge/data" style="color: blue;">Deepfake Detection Challenge (DFDC)</a>
              on Kaggle [4]. More specifically, our team focused on a smaller subset of the data, which was around 10GB, whereas the entire dataset was a whopping 472GB, far too big for the scale of this project.
            </p>
            <h5 class="title">General Data Exploration</h5>
            <p>
                Let us start by analyzing the distribution of the dataset. From Figure 1, we can see that we have a skewed dataset
                with mostly fake videos. This is because the fake videos in this dataset were originally generated from real videos, therefore
                for each real video, there are potentially multiple fake videos that were generated using the real as a base.
                By analyzing the dataset, we were able to find out that each real video was used to generate, on average, 4.4 fake videos. 
            </p>
            <figure>
                <img src="./static/images/data_prop.png" alt="Data Prop" style="width:50%">
                <figcaption>Figure 1 - Proportion of each class in the dataset</figcaption>
            </figure>
            <h5 class="title">Single Frame Exploration</h5>
            <p>
              Let us take a closer look at the dataset now, by extracting specific frames of the video.
              Below are three frames from randomly chosen videos from each class.
          </p>
          <figure>
              <img src="./static/images/reals.png" alt="Reals" style="width:100%">
              <figcaption>Figure 2 - Single frames from real videos</figcaption>
          </figure>
          <figure>
            <img src="./static/images/fakes.png" alt="Fakes" style="width:100%">
            <figcaption>Figure 3 - Single frames from fake videos</figcaption>
          </figure>
          <h5 class="title">Consecutive Multi Frame Exploration</h5>
          <p>
            Unfortunately, we are not able to tell much from still images. Where AI struggles is maintaining consistency across frames in a video.
            Let us now slice into the videos and analyze a subset of frames at a time. To the left is a real video, whereas the right one is fake.
            Just from here, we are able to notice quite a bit. For instance, the face in the fake video is blurry and lacks detail, to the point
            where it even cuts off the person's glasses.
          </p>
        <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
          <video width="400" height="250" controls>
            <source src="./static/videos/real.mp4" type="video/mp4">
          </video>
          <video width="400" height="250" controls>
            <source src="./static/videos/fake.mp4" type="video/mp4">
          </video>
        </div>
        <h5 class="title" style="margin-top: 25px;">Haar Cascade Face Detection and Exploration</h5>
        <p>
          We can see that most of the inconsistency is around the face, which is usually the most difficult for an AI to get right. Let us zoom into the 
          faces in these videos. To do this, we will utilize an object detection algorithm called Haar Cascades Classifier.
        </p>
        <p>
          Notice the variations and expressiveness of the real video, as compared to the blank and emotionless face in the fake one.
        </p>
        <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
          <video width="400" height="250" controls>
            <source src="./static/videos/real_face.mp4" type="video/mp4">
          </video>
          <video width="400" height="250" controls>
            <source src="./static/videos/fake_face.mp4" type="video/mp4">
          </video>
        </div>
        <h5 class="title" style="margin-top: 25px;">Structural Similarity (SSIM) Index</h5>
        <p>
          Now that we know this fact about inconsistency, let us try to quantify it by making use of a metric called the Structural Similarity (SSIM) Index.
          Unfortunately, there does not seem to be a clear indicator that fake videos have a lower SSIM score, as both the real and fake ones seem to be quite random.
        </p>
        <figure>
          <img src="./static/images/ssim.png" alt="SSIM" style="width:100%">
          <figcaption>Figure 4 - SSIM score between consecutive frames</figcaption>
        </figure>
        <p>
          Now that we have explored the dataset, we can start preprocessing the videos to be trained.
          We will look into that in the next section.
        </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End EDA -->
  
  <!-- Methods -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <h3 class="title">Data Preprocessing</h3>
          <p>
            We implemented two different methods for pre-processing. The first method involves the use of Mask R-CNN to detect people in the video. The second method involves the use of convolutional autoencoders to reduce the dimensionality of the data. We did not end up using data augmentation for pre-processing the videos since we arrived at the conclusion that it did not significantly improve the performance of the models.
          </p>
          <h5 class="title">Mask R-CNN Implementation</h5>
          <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
            <video width="400" height="250" controls>
              <source src="./static/videos/maskrcnn_before.mp4" type="video/mp4">
            </video>
            <video width="250" height="160" controls>
              <source src="./static/videos/maskrcnn_after.mp4" type="video/mp4">
            </video>
          </div>
          <br>
          <p>
            To reduce dataset size, we utilized PyTorch's implementation of Mask R-CNN (torchvision.models.detection.mask_rcnn.MaskRCNN). This allowed us to detect areas of frames in which people are present via pixel-level object segmentation and crop the frames accordingly. We used the videos that were generated using these cropped frames as input to our deepfake detection models. The Mask R-CNN model was initialized with weights obtained by pre-training on the COCO dataset. A sample of the video output (mask and bounding box included for reference) of the Mask R-CNN model is shown below. The video on the left is the original video, and the video on the right is the output of the Mask R-CNN model. On this particular video, the Mask R-CNN model yielded a 75% reduction in size. For simplicity, our current Mask R-CNN implementation is configured to detect only a single person in each frame. A potential improvement would be to configure the model to detect multiple people in each frame and modify our models to handle this additional information and make more informed decisions.
          </p>
          <h5 class="title">Convolutional Autoencoder Implementation</h5>
          <p>
            We also implemented a convolutional autoencoder to reduce the dimensionality of the data. 
          </p>
          <br>
          <h3 class="title">Deepfake Classification Models</h3>
          <p>
            We implemented three different deepfake classification techniques. The first involves the use of a support vector machine (SVM) trained on features extracted by feature point detectors. The second involves the use of a convolutional neural network (CNN) architecture called EfficientNet combined with a Vision Transformer. We will also implement a CNN ensemble model that utilizes two different CNN architectures combined with a meta-learner.
          </p>
          <h5 class="title">Support Vector Machine Implementation</h5>
          <figure>
            <img src="./static/images/flowchat.png" style="width:100%">
            <figcaption>Figure 5 - Flowchart of SVM process</figcaption>
          </figure>
          <p>
            For our first model, we utilized a support vector machine [5]. Once we finished the model, we created a visualization to better illustrate the steps and our data processing. We incorporated a flowchart that shows the process of the data preprocessing to feeding the information into the model.
          </p>
          <p>
            The scikit-learn SVM module includes multiple Support Vector Machine algorithms such as Linear SVM, Nu-Spport Vector and C-Support Vector classification. We chose to use C-Support Vector classification with a radial basis function (RBF) kernel for our implementation. The C parameter is a regularization parameter and can be tuned to manage the trade-off between smooth transition boundaries and accurate categorization of training samples. There are several other hyperparameters such as degree, which limit the degree of the polynomial kernel function. We used the default value of 3 for degree. We used the default value of "scale" for the gamma hyperparameter, which controls the kernel coefficient. For completeness, the other model hyperparameters that can be tuned include coef0, shrinking, tol, cache_size, class_weight, max_iter, decision_function_shape, break_ties, and random_state. The results of the model are presented in the following section.
          </p>
          <h5 class="title">EfficientNet-ViT Implementation</h5>
          <p>
            The second model we implemented is EfficientNet combined with a Vision Transformer (ViT) [6]. We pivoted from our original decision to implement a model solely based on ViTs owing to the need to capture localized information in video frames. We chose to use EfficientNet owing to the ability of convolutional neural networks to detect patch abnormalities in frames. We further chose to pass the output of EfficientNet through a ViT owing to the ability of transformers to capture long-range video dependencies. EfficientNet processes the input video frames by extracting features for each 7x7 block in a particular frame. Following linear projection, ViT processes the features for each location generated by EfficientNet [6]. We used the smallest model in the EfficientNet family (EfficientNet B0). We leveraged PyTorch's implementation of this model (torchvision.models.efficientnet_b0) and initialized it with pre-trained weights for the training process. The last few layers of the model were fine-tuned on a subset of the DFDC dataset. The ViT portion of the model was implemented using PyTorch and NumPy. The CLS token is used prior to passing extracted features to the MLP head in order to generate a binary classification score.
          </p>
          <figure>
            <img src="./static/images/efficientnet/efficientnet_architecture.png" alt="EfficientNet-ViT Architecture" style="width:60%">
            <figcaption>Figure 6 - EfficientNet-ViT architecture [6]</figcaption>
          </figure>
          <p>
            The dataset was split into training, validation, and test sets using a 60-20-20 split. Since the dataset contained a limited number of real videos and we wanted to ensure that the model was trained on a balanced dataset, we were limited to approximately 520 videos for training although the training set contained about 1400 videos. We were further limited by CPU memory constraints and had to reduce the training set size to 300 videos (150 real and 150 fake). We extracted 30 frames per video for training, yielding 4500 real frames and 4500 fake frames. The model was trained in a supervised manner on these frames for 200 epochs using binary cross-entropy loss. We performed two rounds of experiments - one involved no data preprocessing while the other involved applying both Mask R-CNN and stacked convolutional autoencoders to the dataset. The specific hyperparameters used for model training are presented in the following table. We performed all experiments on login nodes of NERSC Perlmutter, a supercomputer with 64-core AMD EPYC 7763 CPUs and NVIDIA A100 GPUs (PACE ICE limited the dataset size even further). The inference procedure and results for both sets of experiments are presented in the following section.
          </p>
          <table class="styled-table">
            <tr>
              <th>Hyperparameter</th>
              <th>Value</th>
            </tr>
            <tr>
              <td>Batch Size</td>
              <td>16</td>
            </tr>
            <tr>
              <td>Learning Rate</td>
              <td>0.005</td>
            </tr>
            <tr>
              <td>Optimizer</td>
              <td>SGD</td>
            </tr>
            <tr>
              <td>Patch Size</td>
              <td>7</td>
            </tr>
            <tr>
              <td>Number of Heads</td>
              <td>16</td>
            </tr>
          </table>

	<h5 class="title"> Ensemble Implementation</h5>
		<p>The third model that our team created is an Ensemble with a metalearner model. It combines two models a InceptionResNetV2 Model and the DenseNet121 model. Both of these CNNs are then used to create a stack and a meta learner, in our case a LightGBM model is used [10]. We used a very similar approach to the architecture that is described in the DeepFakeStack, using the same models but just condensing it to fit with less models due to the constraint of the project’s timeline [10]. We start by first feeding 200 real videos and 200 deepfake videos as our training set. Then the program downloads the frame at every 7 seconds of video duration. For both the fake and real data points. Next, the frames are resized to (299, 299, 3) for the InceptionResNet model and (224, 224, 3) for the DenseNet model. We then use 20% of the dataset to be the test data and 80% to be the training data. We use pretrained weights for both the model and utilize binary_crossentropy due to our need for binary classification. Once this was done, we trained the model on 10 epochs with each epoch taking 32 steps through supervised learning. Once we finished that, we got an accuracy of 84% for the InceptionResNet model and an accuracy of 73% for the DenseNet model. After, we downloaded the models and started to set up the LightGBM model as the meta learner. Next we extract the features and combine them into the setup for the LightGBM model. We set up this model to use binary log loss to allow binary classification. After we train this model on 100 boosting rounds. By this we were able to achieve an accuracy of 55% for the overall model. 
 </p>
		<figure>
            <img src="./static/images/ensemble/pipe.jpg" alt="Ensemble Model Architecture" style="width:60%">
            <figcaption>Figure 16 - Ensemble Model architecture [6]</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Methods -->
  
   <!-- Results and Discussion -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results and Discussion</h2>
        <div class="content has-text-justified">
          <p>
            Based on how widely various metrics are used for this task, we chose the following metrics to evaluate model performance [2]:
          </p>
          <ol>
            <li>Area under the ROC curve evaluates binary classifier performance across different classification thresholds [1]. It also quantifies how well the model can differentiate classes [7].</li>
            <li>Precision measures the proportion of positive results within the positive predictions, ensuring that there are few false positives [8]. </li>
            <li>Log loss increases as predictive probability diverges from the actual label [9].</li>
          </ol>
          <p>
            Goal: We aim to detect whether a video is a deepfake accurately by maximizing the first two metrics and minimizing the third.
          </p>
          <p>
            Results: This project expects high accuracy rates and reduction of false positives, where genuine videos are classified as deep fakes. We expect to minimize false negatives, where deepfakes are misclassified as genuine videos.
          </p>	
          <h5 class="title">SVM Results and Analysis</h5>
          <figure>
            <img src="./static/images/ROC-SVM.png" alt="ROC" style="width:100%">
            <figcaption>Figure 7 - ROC curve for SVM</figcaption>
          </figure>
          <figure>
            <img src="./static/images/confusion-matrix-svm.png" alt="confusion-matrix" style="width:100%">
            <figcaption>Figure 8 - Confusion-Matrix for SVM</figcaption>
          </figure>	
          <p>
            We achieved an AUC of 0.289 and a log loss of 0.70. The low AUC suggests the model cannot distinguish between fake and real videos. This is additionally demonstrated in the confusion matrix. The model is not yet performing well since the accuracy is only 59%. The model requires further hyperparameter tuning and an increase in the training dataset size. For instance, when we used 50 videos and split them into 80% training (40 training videos) and 20% testing, the model produced an accuracy of 54.29%. However, when we increased the number of training videos to 60 and split the dataset to 70% training (42 training videos) and 30% testing, the accuracy shot up to 59%. This shows that with even a slight increase in training data, the accuracy improves drastically.
          </p>
          <h5 class="title">EfficientNet-ViT Results and Analysis</h5>
          <figure>
            <img src="./static/images/efficientnet/efficientnet_losses.png" alt="EfficientNet-ViT Training and Validation Losses" style="width:90%">
            <figcaption>Figure 9 - EfficientNet-ViT training and validation loss curves</figcaption>
          </figure>
          <p>
            We first note that the model overfitted on the training dataset as evidenced by the increasing validation loss after epoch 12. Additionally, the training loss did not improve significantly after epoch 30. We trained three different sets of models - one without any preprocessing, one with Mask R-CNN preprocessing, and one with both Mask R-CNN and convolutional autoencoder preprocessing. We found that the model trained on the dataset without any preprocessing (Model 1) performed significantly worse than the model trained on the dataset preprocessed using only Mask R-CNN (Model 2). The model trained on the dataset preprocessed using both Mask R-CNN and convolutional autoencoders (Model 3) did not perform as well as Model 2. We present the training and validation loss curves for Model 2 above.
          </p>
          <p>
            Model 1 achieved a test accuracy of 0.53, a precision of 0.5246, an F1 score of 0.5766, a log loss of 0.7194, and an AUC of 0.582. Model 3 achieved a test accuracy of 0.60, a precision of 0.5758, an F1 score of 0.6552, a log loss of 0.7194, and an AUC of 0.617. For inference on Model 2, we compared the use of two techniques. The first technique involved using the hard voting mechanism discussed by Coccomini et al. (hard voting), wherein a video was classified as a deepfake if the classification score on any one frame exceeded a threshold of 0.55. The second technique involved averaging the ratings on the individual faces in a given video to determine its label. The threshold used for classification was 0.5. Using the first technique, Model 2 achieved a test accuracy of 0.63, a precision of 0.5802, an F1 score of 0.7176, a log loss of 0.6999, and an AUC of 0.737. Using the second technique, Model 2 achieved a test accuracy of 0.73, a precision of 0.7170, an F1 score of 0.7379, a log loss of 0.6621, and an AUC of 0.768. The second technique performed better than the first technique, which is likely due to the fact that the second technique is more robust to noise in the data. The ROC and precision-recall curves for Model 1, both inference procedures on Model 2, and Model 3 are shown below. The results of Model 1, Model 2, and Model 3 are also summarized in the table below and compared against the results achieved by Coccomini et al. [6].
          </p>
          <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
            <figure>
              <img src="./static/images/efficientnet/efficientnet_auc_no_preprocessing.jpg" alt="AUC no pre-processing" style="width:100%">
              <figcaption>Figure 10 - ROC curve for Model 1</figcaption>
            </figure>
            <figure>
              <img src="./static/images/efficientnet/efficientnet_prcurve_no_preprocessing.jpg" alt="PRC no pre-processing" style="width:100%">
              <figcaption>Figure 11 - Precision-Recall curve for Model 1</figcaption>
            </figure>
          </div>
          <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
            <figure>
              <img src="./static/images/efficientnet/efficientnet_auc_hard_voting.jpg" alt="AUC hard voting" style="width:100%">
              <figcaption>Figure 12 - ROC curve for Model 2 with hard voting</figcaption>
            </figure>
            <figure>
              <img src="./static/images/efficientnet/efficientnet_prcurve_hard_voting.jpg" alt="PRC hard voting" style="width:100%">
              <figcaption>Figure 13 - Precision-Recall curve for Model 2 with hard voting</figcaption>
            </figure>
          </div>
          <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
            <figure>
              <img src="./static/images/efficientnet/efficientnet_auc_mean.jpg" alt="AUC mean prediction" style="width:100%">
              <figcaption>Figure 14 - ROC curve for Model 2 with averaging</figcaption>
            </figure>
            <figure>
              <img src="./static/images/efficientnet/efficientnet_prcurve_mean.jpg" alt="PRC mean prediction" style="width:100%">
              <figcaption>Figure 15 - Precision-Recall curve for Model 2 with averaging</figcaption>
            </figure>
          </div>
          <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
            <figure>
              <img src="./static/images/efficientnet/efficientnet_auc_autoencoder.jpg" alt="AUC mean prediction" style="width:100%">
              <figcaption>Figure 16 - ROC curve for Model 3</figcaption>
            </figure>
            <figure>
              <img src="./static/images/efficientnet/efficientnet_prcurve_autoencoder.jpg" alt="PRC mean prediction" style="width:100%">
              <figcaption>Figure 17 - Precision-Recall curve for Model 3</figcaption>
            </figure>
          </div>
          <table class="styled-table">
            <tr>
              <th>Model</th>
              <th>Accuracy</th>
              <th>Precision</th>
              <th>F1 Score</th>
              <th>Log Loss</th>
              <th>AUC</th>
            </tr>
            <tr>
              <td>Model 1</td>
              <td>0.53</td>
              <td>0.5246</td>
              <td>0.5766</td>
              <td>0.7202</td>
              <td>0.582</td>
            </tr>
            <tr>
              <td>Model 2 (Hard Voting)</td>
              <td>0.63</td>
              <td>0.5802</td>
              <td>0.7176</td>
              <td>0.6999</td>
              <td>0.737</td>
            </tr>
            <tr>
              <td>Model 2 (Mean)</td>
              <td>0.73</td>
              <td>0.7170</td>
              <td>0.7379</td>
              <td>0.6621</td>
              <td>0.768</td>
            </tr>
            <tr>
              <td>Model 3</td>
              <td>0.60</td>
              <td>0.5758</td>
              <td>0.6552</td>
              <td>0.7194</td>
              <td>0.617</td>
            </tr>
            <tr>
              <td>Coccomini et al.</td>
              <td>Not Reported</td>
              <td>Not Reported</td>
              <td>0.838</td>
              <td>Not Reported</td>
              <td>0.919</td>
            </tr>
          </table>
            <p>
              Although these results were much better than the model trained for the midterm report (AUC of 0.321), the model can still perform much better with training on a much larger dataset. This is evidenced by the results achieved by Coccomini et al. after training on a dataset of 220,444 faces [6]. However, an AUC of 0.768 is still significant considering that our setting was much more resource-constrained.
            </p>
          <h5 class="title">CNN Ensemble with Meta-Learner Results and Analysis</h5>
          <p>
            The InceptionResNetModel achieved an accuracy of 0.84 and the DenseNet121 achieved an accuracy of 0.73.  Both of these models used the weights provided by imagenet which is what the models included in the paper the architecture is based on [10]. The accuracy of the Ensemble Model with the LightGBM meta filter achieved an accuracy of 0.55 with a AUC-ROC of 0.58. We realized that the individual models ended up producing a lower accuracy than the individual models. After deeper analysis, we found that the reasons included a lack of specification for each model in the ensemble and a lack of diversification. One aspect that the paper we planned this model off of that we did not consider to incorporate, is the inclusion of diverse models that will identify specific features. These would then be combined to provide a prediction that considers many features each processed by a single model that can then provide a narrower prediction. Another issue that presented itself as a result of using similar models is the fact that both models produced similar errors causing for the errors to be more exacerbated. 
          </p>
	<figure>
              <img src="./static/images/ensemble/densenet.jpg" alt="DenseNet121 Confusion Matrix" style="width:100%">
              <figcaption>Figure 18 - DenseNet Confusion Matrix</figcaption>
            </figure>
		<figure>
              <img src="./static/images/ensemble/inception.jpg" alt="InceptionResNetv2 Confusion Matrix" style="width:100%">
              <figcaption>Figure 19 - InceptionResNetv2 Confusion Matrix</figcaption>
            </figure>

	<figure>
              <img src="./static/images/ensemble/ensembleconf.jpg" alt="Ensemble with meta learner Confusion Matrix" style="width:100%">
              <figcaption>Figure 20 - Ensemble Confusion Matrix</figcaption>
            </figure>
	<figure>
              <img src="./static/images/ensemble/aucroc.jpg" alt="AUC-ROC Curve for Ensemble" style="width:100%">
              <figcaption>Figure 21 - AUC-ROC Curve for Ensemble Model</figcaption>
            </figure>
          <br>
          <h3 class="title">Model Comparison</h3>
          <p>
            The SVM model achieved a test accuracy of 59% and an AUC of 0.289. The EfficientNet-ViT model without any preprocessing achieved a test accuracy of 0.53, a precision of 0.5246, an F1 score of 0.5766, a log loss of 0.7202, and an AUC of 0.582, 
		  whereas with preprocessing and using the mean technique it achieved a test accuracy of 0.63, a precision of 0.5802, an F1 score of 0.7176, a log loss of 0.6999, and an AUC of 0.737, and with the hard voting technique, 
		  it achieved a test accuracy of 0.73, a precision of 0.7170, an F1 score of 0.7379, a log loss of 0.6621, and an AUC of 0.768. Finally, the Ensemble Model with the LightGBM meta filter achieved a test accuracy of 0.55 with a AUC-ROC of 0.58.
		  The table below summarizes the results.
          </p>
		<table class="styled-table">
            <tr>
              <th>Model</th>
              <th>Accuracy</th>
              <th>Precision</th>
              <th>F1 Score</th>
              <th>Log Loss</th>
              <th>AUC</th>
            </tr>
		<tr>
              <td>SVM</td>
              <td>0.59</td>
              <td>N/A</td>
              <td>N/A</td>
              <td>0.70</td>
              <td>0.289</td>
            </tr>
            <tr>
              <td>EfficientNet-ViT w/o preprocessing</td>
              <td>0.53</td>
              <td>0.5246</td>
              <td>0.5766</td>
              <td>0.7202</td>
              <td>0.582</td>
            </tr>
            <tr>
              <td>EfficientNet-ViT w/ Mask R-CNN and Autoencoder preprocessing</td>
              <td>0.60</td>
              <td>0.5758</td>
              <td>0.6552</td>
              <td>0.7194</td>
              <td>0.617</td>
            </tr>
            <tr>
              <td>EfficientNet-ViT w/ Mask R-CNN preprocessing (Hard Voting)</td>
              <td>0.63</td>
              <td>0.5802</td>
              <td>0.7176</td>
              <td>0.6999</td>
              <td>0.737</td>
            </tr>
            <tr>
              <td>EfficientNet-ViT w/ Mask R-CNN preprocessing (Mean)</td>
              <td>0.73</td>
              <td>0.7170</td>
              <td>0.7379</td>
              <td>0.6621</td>
              <td>0.768</td>
            </tr>
            <tr>
              <td>CNN Ensemble</td>
              <td>0.55</td>
              <td>0.5423</td>
              <td>0.5923</td>
              <td>0.7201</td>
              <td>0.58</td>
            </tr>
          </table>
		<p>
			The EfficientNet-ViT model w/ Mask R-CNN preprocessing (Mean) provided the best results and it can be attributed to the fact that it is arguably one of the more powerful models we trained, but it is still far from the performance achieved by some of the research papers and some existing models our team explored. One explanation for this is the lack of compute power at our disposal. As mentioned in the previous sections, we were limited to using a small subset of the data to train our models as a result of not having a sufficient amount of compute. Even with PACE ICE, we were running into training issues, such as memory constraints.
		</p>
		<p>
			We hypothesize that by simply training the model on more data, we will be able to achieve an accuracy comparable to the research papers. However, it is important to note that those models were likely developed and trained by several machine learning experts with far greater resources and time at their disposal. As such, in this context, the results we achieved are quite strong. In the section below, we consider some of the ways we can potentially improve the models and get one step closer to tackling the problem of the proliferation of deepfakes.
		</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Results and Discussion -->

  <!-- Next Steps -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Next Steps</h2>
          <div class="content has-text-justified">
            <p>
              <b>PLEASE UPDATE: We can probably remove this depending on the auto-encoder implementation</b>
							We also plan to utilize stacked denoising autoencoders to yield better representations of the inputs. Autoencoders are neural nets that are usually trained to reduce the dimensionality of inputs and then use an inverse transform to recover an output that is as close to the input as possible. This process allows for the learning of a condensed representation of the data. Stacking multiple of these layers improves this process. While autoencoders are usually used to reduce the dimensionality of the dataset, denoising autoencoders learn a higher dimensional representation. This introduces noise, and denoising autoencoders leverage the process of reconstructing the input through the noise to gain a better understanding of the data [12].
            </p>
            <p>
              <b>PLEASE UPDATE: Next steps for SVM implementation cannot talk about PACE</b>
              As far as the SVM implementation goes, personal compute resources limited the training data size. Using PACE ICE will reduce the training time and allow the model to handle a larger training dataset to improve its performance. When we saved the trained model to disk in pickle format, it occupied more than 24 GB. This explains how resource intensive these experiments were. By using dedicated compute machines, the process could be hastened significantly. 
            </p>
            <p>
              We tried several different combinations of hyperparameters in order to improve the performance of our EfficientNet-ViT implementation on unseen data and also incorporated temporal data into the training set to allow for the ViT part of the model to capture long-range dependencies. Although we ended up achieving a decent AUC of 0.768, we ultimately suffered from overfitting owing to the lack of training data. Despite PACE and Perlmutter being powerful supercomputers, the training process was still quite slow and was bottlenecked by memory constraints. At this stage, we believe the only way to improve the model's performance is to use a dataset that is orders of magnitude larger than the subset of DFDC we used.
            </p>
            <p>
              <b>Next steps for CNN Ensemble</b>
              Some of the next steps we could take to improve the accuracy of the CNN Ensemble Model with Meta Learner would be to include more models that pick up on specific features instead of having each model in the ensemble scan to see if the image is a deepfake. We can instead include a larger amount of models that each serve a specific purpose. For example, have a model that focuses on texture recognition, shape recognition, color recognition, and pattern recognition [10]. We could also include more parameters with the training of the model to reduce overfitting. Also, identify the errors made by each model and try to reduce the errors. Finally, the meta learner could also be improved by creating an ensemble model and training it on more data. With a meta-learner that includes more models again more features can be focused on by the model leading to more specific findings by the model that can be identified by the user [10]. Training the models on more data could also provide it with more information to base the predictions off of. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Next Steps -->
  
   <!-- References -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">References</h2>
        <div class="content has-text-justified">
          <p>
            [1] E. Altuncu, V. Franqueira and S. Li, "Deepfake: Definitions, Performance Metrics and Standards, Datasets and Benchmarks, and a Meta-Review," arXiv:2208.10913 [cs.CV].
          </p>
          <p>
            [2] M. S. Rana, M. N. Nobi, B. Murali and A. H. Sung, "Deepfake Detection: A Systematic Literature Review," in IEEE Access, vol. 10, pp. 25494-25513, 2022, doi: 10.1109/ACCESS.2022.3154404.  
          </p>
          <p>
            [3] Y. Nirkin, L. Wolf, Y. Keller and T. Hassner, "DeepFake Detection Based on Discrepancies Between Faces and Their Context," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 6111-6121, 1 Oct. 2022, doi: 10.1109/TPAMI.2021.3093446.  
          </p>
          <p>
            [4] Brian Dolhansky, et al., "The DeepFake Detection Challenge (DFDC) Dataset," arXiv:2006.07397 [cs.CV].
          </p>
          <p>
            [5] F. F. Kharbat, T. Elamsy, A. Mahmoud and R. Abdullah, "Image Feature Detectors for Deepfake Video Detection," 2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA), Abu Dhabi, United Arab Emirates, 2019, pp. 1-4, doi: 10.1109/AICCSA47632.2019.9035360.
          </p>
          <p>
            [6] D. A. Coccomini, N. Messina, C. Gennaro, and F. Falchi, “Combining EfficientNet and vision transformers for video deepfake detection,” Image Analysis and Processing - ICIAP 2022, pp. 219-229, 2022. doi:10.1007/978-3-031-06433-3_19 
          </p>
          <p>
						[7] M. Norton and S. Uryasev, "Maximization of AUC and Buffered AUC in binary classification," Mathematical Programming, vol. 174, no. 1-2, pp. 575-612, Jul. 2018, doi: https://doi.org/10.1007/s10107-018-1312-2.
					</p>
          <p>
						[8] M. Groh , Z. Epstein, C. Firestone, and R. Picard, "Deepfake detection by human crowds, machines, and machine-informed crowds," pnas, https://www.pnas.org/doi/10.1073/pnas.2110013119.
					</p>
					<p>
						[9] S. A. Khan and D.-T. Dang-Nguyen, "Deepfake Detection: A Comparative Analysis," arXiv.org e-Print archive, https://arxiv.org/.
					</p>
          <p>
            [10] Md. S. Rana and A. H. Sung, “DeepfakeStack: A Deep Ensemble-based learning technique for deepfake detection,” 2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom), Aug. 2020. doi:10.1109/cscloud-edgecom49738.2020.00021 
          </p>
          <p>
            [11] K. Yao, J. Wang, B. Diao, and C. Li, “Towards understanding the generalization of deepfake detectors from a game-theoretical view,” 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Oct. 2023. doi:10.1109/iccv51070.2023.00194 
          </p>
          <p>
            [12] J. Liang and K. Kelly, "Training Stacked Denoising Autoencoders for Representation Learning," arXiv:2102.08012 [cs.CV].
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End References -->
  
  <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Contribution Table</h2>
      <div class="columns is-centered has-text-centered">
  <table class="styled-table">
  <tr>
    <th>Name</th>
    <th>Final Contributions</th>
  </tr>
  <tr>
    <td>Sagar Sadak</td>
    <td>Introduction, Problem Description, Exploratory Data Analysis and Visualization, Model Comparison</td>
  </tr>
  <tr>
    <td>Shaam Bala</td>
    <td>Introduction, Problem Description, Next Steps, Visualizations, CNN Ensemble Model with Meta-learner Implementation</td>
  </tr>
    <tr>
    <td>Akaash Parthasarathy</td>
    <td>Methods, Next Steps, EfficientNet-ViT Implementation and Discussion, Mask R-CNN Implementation</td>
  </tr>
    <tr>
    <td>Akshath Shvetang Anna</td>
    <td>Next Steps, Stacked Denoising Autoencoders Implementation</td>
  </tr>
    <tr>
    <td>Harish Rajan</td>
    <td>SVM Implementation and Visualizations, Results and Discussion for SVM</td>
  </tr>
</table>
      </div>
    </div>
  </div>
</section>
</body>
</html>
