<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>
  
  
    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="deepfakes, computer vision, speech recognition">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  
  
    <title>Team 6 Project Proposal</title>
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
  
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>


<!-- Paper Intro -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction/Background</h2>
        <div class="content has-text-justified">
          <p>
            "Deepfakes" encompasses a wide range of media created from the manipulation of existing sources and/or the creation of new content using advanced technologies. They can be used for malicious purposes, motivating the need to be able to identify deepfakes [1].
          </p>
          <p>
            Some approaches to deepfake detection include machine learning-based methods, deep learning-based methods (~77% of all research), statistical-based methods, and blockchain-based methods [2]. The main process of detecting deepfakes entails two parts: assessing how the face is integrated with its background and how the background blends with the face [3].
          </p>
          <p>
            We will primarily use the <a href="https://www.kaggle.com/c/deepfake-detection-challenge/data">DFDC dataset</a>, which includes deepfakes involving the manipulation of human facial images and voices [4].
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Intro -->
  
  <!-- Problem Definition -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Definition</h2>
        <div class="content has-text-justified">
          <p>
            The growing proliferation of deepfake technology poses various risks, including identity theft and the spread of misinformation. Our team’s motivation thus stems from the dire necessity for people to reliably distinguish between fiction and reality. By developing an effective model that can classify, with high AUC/ROC, whether videos are authentic or AI-generated, we aim to combat the aforementioned issues.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Problem Definition -->

 <!-- EDA -->
 <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Exploratory Data Analysis and Visualizations</h2>
          <div class="content has-text-justified">
            <h5 class="title">Dataset</h5>
            <p>
              The dataset used in this project was sourced from the <a href="https://www.kaggle.com/competitions/deepfake-detection-challenge/data" style="color: blue;">Deepfake Detection Challenge (DFDC)</a>
              on Kaggle. More specifically, our team focused on a smaller subset of the data, which was around 10GB, whereas the entire dataset was a whopping 472GB, far too big for the scale of this project.
            </p>
            <h5 class="title">General Data Exploration</h5>
            <p>
                Let us start by analyzing the distribution of the dataset. From Figure 1, we can see that we have a skewed dataset
                with mostly fake videos. This is because the fake videos in this dataset were originally generated from real videos, therefore
                for each real video, there are potentially multiple fake videos that were generated using the real as a base.
                By analyzing the dataset, we were able to find out that each real video was used to generate, on average, 4.4 fake videos. 
            </p>
            <figure>
                <img src="./static/images/data_prop.png" alt="Data Prop" style="width:50%">
                <figcaption>Figure 1 - Proportion of each class in the dataset</figcaption>
            </figure>
            <h5 class="title">Single Frame Exploration</h5>
            <p>
              Let us take a closer look at the dataset now, by extracting specific frames of the video.
              Below are three frames from randomly chosen videos from each class.
          </p>
          <figure>
              <img src="./static/images/reals.png" alt="Reals" style="width:100%">
              <figcaption>Figure 2 - Single frames from real videos</figcaption>
          </figure>
          <figure>
            <img src="./static/images/fakes.png" alt="Fakes" style="width:100%">
            <figcaption>Figure 3 - Single frames from fake videos</figcaption>
          </figure>
          <h5 class="title">Consecutive Multi Frame Exploration</h5>
          <p>
            Unfortunately, we are not able to tell much from still images. Where AI struggles is maintaining consistency across frames in a video.
            Let us now slice into the videos and analyze a subset of frames at a time. To the left is a real video, whereas the right one is fake.
            Just from here, we are able to notice quite a bit. For instance, the face in the fake video is blurry and lacks detail, to the point
            where it even cuts off the person's glasses.
          </p>
          <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
          <video width="400" height="250" controls>
            <source src="./static/videos/real.mp4" type="video/mp4">
          </video>
          <video width="400" height="250" controls>
            <source src="./static/videos/fake.mp4" type="video/mp4">
          </video>
        </div>
        <h5 class="title" style="margin-top: 25px;">Haar Cascade Face Detection and Exploration</h5>
        <p>
          We can see that most of the inconsistency is around the face, which is usually the most difficult for an AI to get right. Let us zoom into the 
          faces in these videos. To do this, we will utilize an object detection algorithm called Haar Cascades Classifier.
        </p>
        <p>
          Notice the variations and expressiveness of the real video, as compared to the blank and emotionless face in the fake one.
        </p>
        <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
          <video width="400" height="250" controls>
            <source src="./static/videos/real_face.mp4" type="video/mp4">
          </video>
          <video width="400" height="250" controls>
            <source src="./static/videos/fake_face.mp4" type="video/mp4">
          </video>
        </div>
        <h5 class="title" style="margin-top: 25px;">Structural Similarity (SSIM) Index</h5>
        <p>
          Now that we know this fact about inconsistency, let us try to quantify it by making use of a metric called the Structural Similarity (SSIM) Index.
          Unfortunately, there does not seem to be a clear indicator that fake videos have a lower SSIM score, as both the real and fake ones seem to be quite random.
        </p>
        <figure>
          <img src="./static/images/ssim.png" alt="SSIM" style="width:100%">
          <figcaption>Figure 4 - SSIM score between consecutive frames</figcaption>
        </figure>
        <p>
          Now that we have explored the dataset, we can start preprocessing the videos to be trained.
          We will look into that in the next section.
        </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End EDA -->
  
  <!-- Methods -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>
            Our final project submission will include three separate methods for data preprocessing. The first method involves the use of Mask R-CNN to detect people in the video. The second method involves the use of convolutional autoencoders to reduce the dimensionality of the data. The third method involves performing data augmentation through minor video transformations that do not constitute deepfakes.
          </p>
          <p>
            For the midterm checkpoint, we have implemented the first of these pre-processing techniques. To reduce dataset size, we utilized PyTorch's implementation of Mask R-CNN (torchvision.models.detection.mask_rcnn.MaskRCNN). This allowed us to detect areas of frames in which people are present via pixel-level object segmentation and crop the frames accordingly. We used the videos that were generated using these cropped frames as input to our deepfake detection models. The Mask R-CNN model was initialized with weights obtained by pre-training on the COCO dataset. A sample of the video output (mask and bounding box included for reference) of the Mask R-CNN model is shown below.
          </p>
          <div style="display: flex; align-items: center; justify-content: center; gap: 15px;">
            <video width="400" height="250" controls>
              <source src="./static/videos/maskrcnn_before.mp4" type="video/mp4">
            </video>
            <video width="400" height="250" controls>
              <source src="./static/videos/maskrcnn_after.mp4" type="video/mp4">
            </video>
          </div>
          <p>
            Our final project submission will include the implementation of three different deepfake classification techniques. The first involves the use of a support vector machine (SVM) trained on features extracted by feature point detectors. The second involves the use of a convolutional neural network (CNN) architecture called EfficientNet combined with a Vision Transformer. We will also implement a CNN ensemble model that utilizes two different CNN architectures combined with a meta-learner.
          </p>
          <h5 class="title">Support Vector Machine Implementation</h5>
          <figure>
            <img src="./static/images/flowchat.png" style="width:100%">
            <figcaption>Figure 5 - Flowchart of SVM process</figcaption>
          </figure>
          <p>
            For our first model, we utilized a support vector machine. Once we finished the model, we created a visualization to better illustrate the steps and our data processing. We incorporated a flowchart that shows the process of the data preprocessing to feeding the information into the model.
          </p>
          <p>
            The scikit-learn SVM module includes multiple Support Vector Machine algorithms. They include Linear SVM, Nu-Spport Vector and C-Support Vector classification. We chose to use C-Support Vector classification. SVM supports different kernels like linear, polynomial, SGD and many others. We used the default Radial Basis Function (rbf) kernel. C parameter is a regularization parameter and can be tuned to manage the trade-off between smooth transition boundaries and categorizing the training sample more accurately. We did not try multiple experiments to train the model and study the output accuracies. There are numerous other parameters like degree, which limits the degree of the polynomial kernel function. We used the default of 3. The gamma parameter controls kernel coefficient. We used the default value of "scale". There is another possible value "auto". We did not alter to study the impact of that. For completeness, we are listing the other parameters to be tuned: coef0, shrinking, tol, cache_size, class_weight, max_iter, decision_function_shape, break_ties, and random_state. All these could be tuned through multiple iterations to get better results from the model.  
          </p>
          <p>
            We are also in the process of implementing EfficientNet combined with a Vision Transformer (ViT) [10]. We pivoted from our original decision to implement a model solely based on ViTs owing to the need to capture localized information in video frames. In particular, we chose to use EfficientNet owing to the ability of convolutional neural networks to detect patch abnormalities in frames. EfficientNet processes the input video frames by extracting features for each 7x7 block in a particular frame. Following linear projection, ViT processes the features for each location generated by EfficientNet. We used the smallest model in the EfficientNet family (EfficientNet B0). We leveraged PyTorch's implementation of this model (torchvision.models.efficientnet_b0) and initialized it with pre-trained weights for the training process. The last few layers of the model were fine-tuned on a subset of the DFDC dataset. The ViT portion of the model was implemented using PyTorch and NumPy. The CLS token is used prior to passing extracted features to the MLP head in order to generate a binary classification score. 
          </p>
          <p>
            The model was trained in a supervised manner using 100 labeled training examples from DFDC, from which 30 frames were extracted per video. The specific hyperparameters used for model training are presented in the following table. The validation and testing sets were also subsets of DFDC. The inference procedure and results are presented in the following section.
          </p>
          <table class="styled-table">
            <tr>
              <th>Hyperparameter</th>
              <th>Value</th>
            </tr>
            <tr>
              <td>Batch Size</td>
              <td>1</td>
            </tr>
            <tr>
              <td>Learning Rate</td>
              <td>0.001</td>
            </tr>
            <tr>
              <td>Optimizer</td>
              <td>Adam</td>
            <tr>
              <td>Epochs</td>
              <td>20</td>
            </tr>
            <tr>
              <td>Frames Per Video</td>
              <td>30</td>
            </tr>
            <tr>
              <td>Image Size</td>
              <td>224</td>
            </tr>
            <tr>
              <td>Patch Size</td>
              <td>7</td>
            </tr>
            <tr>
              <td>Number of Heads</td>
              <td>8</td>
            </tr>
          </table>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Methods -->
  
   <!-- Results and Discussion -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results and Discussion</h2>
        <div class="content has-text-justified">
          <p>
            Based on how widely various metrics are used for this task, we have chosen the following metrics [2]:
          </p>
          <ol>
            <li>Area under the ROC curve evaluates binary classifier performance across different classification thresholds [1]. It also quantifies how well the model can differentiate classes [11].</li>
            <li>Precision measures the proportion of positive results within the positive predictions, ensuring that there are few false positives [12]. </li>
            <li>Log loss increases as predictive probability diverges from the actual label [13].</li>
          </ol>
	<figure>
          <img src="./static/images/ROC-SVM.png" alt="ROC" style="width:100%">
          <figcaption>Figure 6 - ROC curve for SVM</figcaption>
        </figure>

	<figure>
          <img src="./static/images/confusion-matrix-svm.png" alt="confusion-matrix" style="width:100%">
          <figcaption>Figure 7 - Confusion-Matrix for SVM</figcaption>
        </figure>	
          <p>
            Goal: We aim to detect whether a video is a deepfake accurately by maximizing the first two metrics and minimizing the third.
          </p>
          <p>
            Results: This project expects high accuracy rates and reduction of false positives, where genuine videos are classified as deep fakes. We expect to minimize false negatives, where deepfakes are misclassified as genuine videos.
          <p>	
            Also got a ROC AUC score of 0.289, which means the model is not distinguishing between the fake and real videos and is not performing better. The model needs better hyper parameters tuning and also need to increase the training dataset size. 
          </p>
          <p>
            With an accuracy of 59%, this SVM classification model is not performing so well. And that's because the number of videos used for training is very limited. When I used 50 videos and split the videos into 80% training (40 videos) and 20% testing (10 videos), the model gave an accuracy of 54.29% When I increased the videos to 60 and split the video dataset to 70% (42 videos) for training an 30% (18 videos) for testing, the accuracy shot up to 59%. This proves with more training data, the accuracy can further shoot up and get closer to acceptable levels. Unfortunately, the training of the SVM model took multiple hours and the Python kernel on my mac started crashing even for 60 videos and hence could not go above this number.  
          </p>
          <p>
		        The model is not distinguishing between fake and real classes well, as we can see from ROC AUC score of 0.289. A Log Loss of 0.70 needs a huge improvement as well. Again with the increase in the size of the training dataset, a higher ROC AUC score can be achieved.
          </p>
		      <p>
            First iteration with 100 videos (80%-20% training-test split) 
		        50 real videos and 50 fake videos
            Accuracy Score : 0.5904761904761905
          </p>
          <p>
            For inference for the EfficientNet-ViT model, we used a voting mechanism as discussed by Coccomini et al. [10]. However, rather than grouping scores by actor identifiers, we averaged the ratings on the individual faces in a given video to determine its label. The hard voting approach discussed by Coccomini et al. may be worth exploring for the final project submission [10]. The precision of the model was 0.6364 and the F1 score was 0.7778. The log loss was 0.6769 and the AUC was 0.321. The low precision of the model is likely due to the small training set used for the midterm checkpoint. Additionally, the AUC of 0.321 indicates that the model is currently performing worse than random guessing. The model currently tends toward false positives, which is reflected in the low AUC. The ROC curve and precision-recall curve for the model are shown below. We are confident that training the model on the entire DFDC dataset will help the model better classify negative examples and improve these metrics.
          </p>
          <figure>
            <img src="./static/images/efficientnet_auc.jpg" alt="AUC" style="width:100%">
            <figcaption>Figure 8 - ROC curve for EfficientNet-ViT</figcaption>
          </figure>
          <figure>
            <img src="./static/images/efficientnet_prcurve.jpg" alt="PRC" style="width:100%">
            <figcaption>Figure 9 - Precision-Recall curve for EfficientNet-ViT</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Results and Discussion -->

  <!-- Next Steps -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Next Steps</h2>
          <div class="content has-text-justified">
            <p>
              Our next steps include implementing our third model, the CNN ensemble model with a meta-learner. The new model that we plan to incorporate will require the implementation of the aforementioned data preprocessing techniques as well as a few additional ones in order to modify the input for the models that will be used in the ensemble. The models we plan to train and include in our ensemble will be the InceptionResNetV2 model and the DenseNet121 model [8]. These two models are highly compatible with each other. The InceptionResNetV2 model will handle analyzing the overall image for large incongruencies, whereas the DenseNet121 will look at the image more indepth for subtleties that allude to the video being a deepfake. Once these models are trained, we plan to utilize a meta-learner to efficiently combine the data from the two models to provide an accurate prediction. The model we will be using to complete this will be the LightGBM gradient boosting model [9]. We have decided on this model owing to its ability to handle large datasets quickly and effectively. Based on this implementation, we should be able to provide a CNN ensemble model with a meta learner that performs well on all the discussed metrics.
            </p>
            <p>
              We also plan to implement the convolutional autoencoder to reduce the dimensionality of the data.
            </p>
            <p>
              Personal mac compute resource also limited the training data size. Using any cloud offering by getting higher compute resources including more cores and large memory will also reduce the training time and can handle bigger training dataset to improve the accuracies. When we saved the trained model to my disk in pickle format, it was more than 24 GB. It explains how resource sensitive these experiments were. By using high compute machines, the process could be improved significantly. 
            </p>
            <p>
              There are still plenty of improvements to make to the EfficientNet-ViT implementation and training pipeline. Specifically, owing to deadline constraints for the midterm, a very small subset of DFDC was used for training. We plan train the model on PACE ICE using the entire 10GB dataset of videos (and, perhaps, an even larger subset of DFDC) for the final project submission and are confident that this will drastically improve model performance. In addition, the current setup involves passing in the features to the ViT component without any temporal information. We plan to modify the training procedure to incorporate such information so that the ViT can more effectively incorporate long-range dependencies.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Next Steps -->
  
   <!-- References -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">References</h2>
        <div class="content has-text-justified">
          <p>
          [1] E. Altuncu, V. Franqueira and S. Li, "Deepfake: Definitions, Performance Metrics and Standards, Datasets and Benchmarks, and a Meta-Review," arXiv:2208.10913 [cs.CV].
          </p>
          <p>
          [2] M. S. Rana, M. N. Nobi, B. Murali and A. H. Sung, "Deepfake Detection: A Systematic Literature Review," in IEEE Access, vol. 10, pp. 25494-25513, 2022, doi: 10.1109/ACCESS.2022.3154404.  
          </p>
          <p>
          [3] Y. Nirkin, L. Wolf, Y. Keller and T. Hassner, "DeepFake Detection Based on Discrepancies Between Faces and Their Context," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 6111-6121, 1 Oct. 2022, doi: 10.1109/TPAMI.2021.3093446.  
          </p>
          <p>
            [4] Brian Dolhansky, et al., "The DeepFake Detection Challenge (DFDC) Dataset," arXiv:2006.07397 [cs.CV].
          </p>
          <p>
            [5] Y. Li and S. Lyu, "Exposing deepfake videos by detecting face warping artifacts," arXiv:1811.00656 [cs.CV].
          </p>
          <p>
            [6] J. Liang and K. Kelly, "Training Stacked Denoising Autoencoders for Representation Learning," arXiv:2102.08012 [cs.CV].
          </p>
          <p>
            [7] T. Kinnunen, E. Chernenko, M. Tuononen, P. Fränti, and H. Li, "Voice Activity Detection Using MFCC Features and Support Vector Machine," Semantic Scholar, 2007. https://www.semanticscholar.org/paper/Voice-Activity-Detection-Using-MFCC-Features-and-Kinnunen-Chernenko/a31eeed2b395e923d3c077d5fedbecfd7f381dd2.
          </p>
          <p>
            [8] Md. S. Rana and A. H. Sung, “DeepfakeStack: A Deep Ensemble-based learning technique for deepfake detection,” 2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom), Aug. 2020. doi:10.1109/cscloud-edgecom49738.2020.00021 
          </p>
          <p>
            [9] K. Yao, J. Wang, B. Diao, and C. Li, “Towards understanding the generalization of deepfake detectors from a game-theoretical view,” 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Oct. 2023. doi:10.1109/iccv51070.2023.00194 
          </p>
          <p>
            [10] D. A. Coccomini, N. Messina, C. Gennaro, and F. Falchi, “Combining EfficientNet and vision transformers for video deepfake detection,” Image Analysis and Processing - ICIAP 2022, pp. 219-229, 2022. doi:10.1007/978-3-031-06433-3_19 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End References -->
  
  <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Contribution Table</h2>
      <div class="columns is-centered has-text-centered">
  <table class="styled-table">
  <tr>
    <th>Name</th>
    <th>Proposal Contributions</th>
  </tr>
  <tr>
    <td>Sagar Sadak</td>
    <td>Report, EDA, Data Visualization</td>
  </tr>
  <tr>
    <td>Shaam Bala</td>
    <td>Introduction, Problem description, Methods, Next Steps, Visualizations, CNN Ensemble Model with meta-learner implementation</td>
  </tr>
    <tr>
    <td>Akaash Parthasarathy</td>
    <td>Methods, MaskRCNN</td>
  </tr>
    <tr>
    <td>Akshath Shvetang Anna</td>
    <td>Next Steps, Autoencoders</td>
  </tr>
    <tr>
    <td>Harish Rajan</td>
    <td> Full Implementation of SVM with Data Preprocessing and Data Visualization, Results and Discussion for SVM</td>
  </tr>
</table>
      </div>
    </div>
  </div>
</section>
</body>
</html>
